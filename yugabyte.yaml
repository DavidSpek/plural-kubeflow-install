---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/service.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: yugabyte-yb-masters-pdb
spec:
  maxUnavailable: 1
  selector:
    matchLabels:      
      app.kubernetes.io/name: "yb-master"
      release: "yugabyte"
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/service.yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: yugabyte-yb-tservers-pdb
spec:
  maxUnavailable: 1
  selector:
    matchLabels:      
      app.kubernetes.io/name: "yb-tserver"
      release: "yugabyte"
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: yugabyte
  labels:    
    heritage: "Helm"
    release: "yugabyte"
    chart: "yugabyte-2.15.1"
    component: "yugabytedb"
---
# Source: yugabyte/charts/yugabyte/charts/oidc-config/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: oidc-config
type: Opaque
stringData:
  OAUTH2_PROXY_PROVIDER: oidc
  OAUTH2_PROXY_HTTP_ADDRESS: 0.0.0.0:4180
  OAUTH2_PROXY_METRICS_ADDRESS: 0.0.0.0:44180
  OAUTH2_PROXY_COOKIE_EXPIRE: 48h
  OAUTH2_PROXY_COOKIE_REFRESH: 24h
  OAUTH2_PROXY_COOKIE_SECURE: "true"
  OAUTH2_PROXY_COOKIE_NAME: _oauth2_proxy
  OAUTH2_PROXY_COOKIE_SAMESITE: lax
  OAUTH2_PROXY_EMAIL_DOMAINS: "*"
  OAUTH2_PROXY_OIDC_ISSUER_URL: PLACEHOLDER
  OAUTH2_PROXY_PASS_ACCESS_TOKEN: "true"
  OAUTH2_PROXY_SCOPE: "openid profile"
  OAUTH2_PROXY_SET_AUTHORIZATION_HEADER: "true"
  OAUTH2_PROXY_SET_XAUTHREQUEST: "true"
  OAUTH2_PROXY_SKIP_PROVIDER_BUTTON: "true"
  OAUTH2_PROXY_UPSTREAMS: http://localhost:80
  OAUTH2_PROXY_USER_ID_CLAIM: email
  OAUTH2_PROXY_CLIENT_ID: PLACEHOLDER
  OAUTH2_PROXY_CLIENT_SECRET: PLACEHOLDER
  OAUTH2_PROXY_COOKIE_SECRET: PLACEHOLDER
---
# Source: yugabyte/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: plural-license-secret
stringData:
  license: _RI0KuFwLDBif7t1Q9X0tg4DE5oSumuNPmIx6vucEl8=
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/setup-auto-multi-az-configmaps.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: yugabyte-setup-auto-multi-az-script
  namespace: "yugabyte"
  labels:
    release: "yugabyte"
    chart: "yugabyte"
    component: "yugabytedb"
data:
  setup-auto-multi-az.sh: |
    #!/bin/bash

    set -eo pipefail

    
    echo "Waiting for Yugabyte pod 0 to be running"
    kubectl wait --for=jsonpath='{.status.phase}'=Running -n yugabyte --timeout 10m pod/yugabyte-yb-master-0
    
    echo "Waiting for Yugabyte pod 1 to be running"
    kubectl wait --for=jsonpath='{.status.phase}'=Running -n yugabyte --timeout 10m pod/yugabyte-yb-master-1
    
    echo "Waiting for Yugabyte pod 2 to be running"
    kubectl wait --for=jsonpath='{.status.phase}'=Running -n yugabyte --timeout 10m pod/yugabyte-yb-master-2
    

    # Wait for Yugabyte to become responsive
    sleep 10

    echo "Getting info about masters\n"

    json=$(curl yugabyte-yb-masters:7000/api/v1/masters)

    until [ $json != "{}" ]
    do
      echo "waiting for master info to be available"
      sleep 5
      json=$(curl yugabyte-yb-masters:7000/api/v1/masters)
    done

    length=$(echo $json | jq ".masters | length")

    placement_strings=$(echo $json | jq --raw-output '.masters[] | "\(.registration.cloud_info.placement_cloud).\(.registration.cloud_info.placement_region).\(.registration.cloud_info.placement_zone)"')
    placement_string=$(echo $placement_strings | sed -e "s/ /,/g")

    master_addresses=$(echo $json | jq --raw-output '.masters[] | "\(.registration.private_rpc_addresses[].host):\(.registration.private_rpc_addresses[].port)"')
    master_addresses_string=$(echo $master_addresses | sed -e "s/ /,/g")

    echo "Configuring zone-aware replica placement\n"

    kubectl exec -i -n yugabyte yugabyte-yb-master-0 -- bash \
      -c "/home/yugabyte/master/bin/yb-admin \
        --certs_dir_name=/opt/certs/yugabyte \
        --master_addresses $master_addresses_string modify_placement_info $placement_string $length"
  get-node-info.sh: |
    #!/bin/bash

    set -eo pipefail

    TOKEN=$(cat ${SERVICEACCOUNT}/token)
    CACERT=${SERVICEACCOUNT}/ca.crt

    # Get node labels into a json
    curl -s --cacert ${CACERT} \
        --header "Authorization: Bearer ${TOKEN}" \
        -X GET ${APISERVER}/api/v1/nodes/${NODENAME} | jq .metadata.labels > /node/labels.json

    # Extract 'topology.kubernetes.io/zone' and 'topology.kubernetes.io/region' from json
    NODE_ZONE=$(jq '."topology.kubernetes.io/zone"' -r /node/labels.json)
    NODE_REGION=$(jq '."topology.kubernetes.io/region"' -r /node/labels.json)

    # and save it into a file in the format suitable for sourcing
    echo "export NODE_ZONE=${NODE_ZONE}" > /node/location
    echo "export NODE_REGION=${NODE_REGION}" >> /node/location
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/setup-auto-multi-az-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: describe-nodes
  labels:    
    heritage: "Helm"
    release: "yugabyte"
    chart: "yugabyte-2.15.1"
    component: "yugabytedb"
rules:
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get"]
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/setup-auto-multi-az-rbac.yaml
# Associate the cluster role with the service account
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: describe-nodes
  labels:    
    heritage: "Helm"
    release: "yugabyte"
    chart: "yugabyte-2.15.1"
    component: "yugabytedb"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: describe-nodes
subjects:
- kind: ServiceAccount
  name: yugabyte
  namespace: yugabyte
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/setup-auto-multi-az-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: exec-pods
  labels:    
    heritage: "Helm"
    release: "yugabyte"
    chart: "yugabyte-2.15.1"
    component: "yugabytedb"
rules:
  - apiGroups: [""]
    resources: ["pods"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    verbs: [create]
    resources: [pods/exec]
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/setup-auto-multi-az-rbac.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: exec-pods
  labels:    
    heritage: "Helm"
    release: "yugabyte"
    chart: "yugabyte-2.15.1"
    component: "yugabytedb"
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: exec-pods
subjects:
- kind: ServiceAccount
  name: yugabyte
  namespace: yugabyte
---
# Source: yugabyte/charts/yugabyte/charts/oidc-config/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: oauth-proxy
  labels:
    endpoint: oauth2-proxy
    helm.sh/chart: oidc-config-0.1.6
    app.kubernetes.io/name: oidc-config
    app.kubernetes.io/instance: yugabyte
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
  - port: 80
    targetPort: 4180
    protocol: TCP
    name: http-oauth
  - port: 44180
    targetPort: 44180
    protocol: TCP
    name: metrics-oauth
  selector:
    
    {}
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/service.yaml
---
apiVersion: v1
kind: Service
metadata:
  name: "yugabyte-yb-masters"
  labels:    
    app.kubernetes.io/name: "yb-master"    
    heritage: "Helm"
    release: "yugabyte"
    chart: "yugabyte-2.15.1"
    component: "yugabytedb"
    service-type: "headless"
spec:
  clusterIP: None
  ports:
    - name: "http-ui"
      port: 7000
    - name: "tcp-rpc-port"
      port: 7100
  selector:    
    app.kubernetes.io/name: "yb-master"
    release: "yugabyte"
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: "yugabyte-yb-tservers"
  labels:    
    app.kubernetes.io/name: "yb-tserver"    
    heritage: "Helm"
    release: "yugabyte"
    chart: "yugabyte-2.15.1"
    component: "yugabytedb"
    service-type: "headless"
spec:
  clusterIP: None
  ports:
    - name: "http-ui"
      port: 9000
    - name: "http-ycql-met"
      port: 12000
    - name: "http-yedis-met"
      port: 11000
    - name: "http-ysql-met"
      port: 13000
    - name: "tcp-rpc-port"
      port: 9100
    - name: "tcp-yedis-port"
      port: 6379
    - name: "tcp-yql-port"
      port: 9042
    - name: "tcp-ysql-port"
      port: 5433
  selector:    
    app.kubernetes.io/name: "yb-tserver"
    release: "yugabyte"
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/service.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: "yugabyte-yb-master"
  namespace: "yugabyte"
  labels:    
    app.kubernetes.io/name: "yb-master"    
    heritage: "Helm"
    release: "yugabyte"
    chart: "yugabyte-2.15.1"
    component: "yugabytedb"
spec:
  serviceName: "yugabyte-yb-masters"
  podManagementPolicy: Parallel
  
  replicas: 3
  
  volumeClaimTemplates:
    - metadata:
        name: yugabyte-datadir0
        labels:          
          heritage: "Helm"
          release: "yugabyte"
          chart: "yugabyte-2.15.1"
          component: "yugabytedb"
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: 10Gi
    - metadata:
        name: yugabyte-datadir1
        labels:          
          heritage: "Helm"
          release: "yugabyte"
          chart: "yugabyte-2.15.1"
          component: "yugabytedb"
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: 10Gi
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      
      partition: 0
      
  selector:
    matchLabels:      
      app.kubernetes.io/name: "yb-master"
      release: "yugabyte"
  template:
    metadata:
      annotations:
        checksum/rootCA: 90ace38c3efb4bf1768630295603f06eb0c140b0384c0860e28e0ad1d4dd6a44
      labels:        
        app.kubernetes.io/name: "yb-master"        
        heritage: "Helm"
        release: "yugabyte"
        chart: "yugabyte-2.15.1"
        component: "yugabytedb"
    spec: # yb-masters
      terminationGracePeriodSeconds: 300
      serviceAccountName: yugabyte
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: "yb-master"
            release: "yugabyte"
      affinity:
        # Set the anti-affinity selector scope to YB masters.
        
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - "yb-master"
                - key: release
                  operator: In
                  values:
                  - "yugabyte"
              topologyKey: kubernetes.io/hostname
      initContainers:
      # The container that extracts the labels
      - name: get-node-labels
        image: "bitnami/kubectl:1.24.3"
        # It'll put labels here
        volumeMounts:
          - mountPath: /node
            name: node-info
          - name: setup-auto-multi-az-script
            mountPath: "/home/yugabyte/bin/setup-auto-multi-az"
        env:
          # pass node name to the environment
          - name: NODENAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: APISERVER
            value: https://kubernetes.default.svc
          - name: SERVICEACCOUNT
            value: /var/run/secrets/kubernetes.io/serviceaccount
        command: ["/bin/bash", "/home/yugabyte/bin/setup-auto-multi-az/get-node-info.sh"]
      containers:
      - name: "yb-master"
        image: "yugabytedb/yugabyte:2.15.1.0-b175"
        imagePullPolicy: IfNotPresent
        lifecycle:
          postStart:
            exec:
              command:
                - "bash"
                - "-c"
                - >
                  mkdir -p /mnt/disk0/cores;
                  mkdir -p /mnt/disk0/yb-data/scripts;
                  if [ ! -f /mnt/disk0/yb-data/scripts/log_cleanup.sh ]; then
                    if [ -f /home/yugabyte/bin/log_cleanup.sh ]; then
                      cp /home/yugabyte/bin/log_cleanup.sh /mnt/disk0/yb-data/scripts;
                    fi;
                  fi
        livenessProbe:
          exec:
            command:
            - bash
            - -c
            - touch "/mnt/disk0/disk.check" "/mnt/disk1/disk.check"
          failureThreshold: 3
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: YBDEVOPS_CORECOPY_DIR
          value: "/mnt/disk0/cores"
        resources:
        
          limits:
            cpu: 2
            memory: 2Gi
          requests:
            cpu: 2
            memory: 2Gi
        
        # core dumps are collected to workingDir if
        # kernel.core_pattern is set to a relative path like
        # core.%e.%p.%t ref:
        # https://github.com/yugabyte/charts/issues/11
        workingDir: "/mnt/disk0/cores"
        command:
          - "/sbin/tini"
          - "--"
        args:
          - "/bin/bash"
          - "-c"
          - |
            source /node/location && \
            touch "/mnt/disk0/disk.check" "/mnt/disk1/disk.check" && \
            if [ -f /home/yugabyte/tools/k8s_preflight.py ]; then
              PYTHONUNBUFFERED="true" /home/yugabyte/tools/k8s_preflight.py \
                --addr="$(HOSTNAME).yugabyte-yb-masters.$(NAMESPACE).svc.cluster.local" \
                --port="7100"
            fi && \
            if [ -f /home/yugabyte/tools/k8s_preflight.py ]; then
              PYTHONUNBUFFERED="true" /home/yugabyte/tools/k8s_preflight.py \
                --addr="$(HOSTNAME).yugabyte-yb-masters.$(NAMESPACE).svc.cluster.local:7100" \
                --port="7100"
            fi && \
            if [ -f /home/yugabyte/tools/k8s_preflight.py ]; then
              PYTHONUNBUFFERED="true" /home/yugabyte/tools/k8s_preflight.py \
                --addr="0.0.0.0" \
                --port="7000"
            fi && \
            if [[ -f /home/yugabyte/tools/k8s_parent.py ]]; then
              k8s_parent="/home/yugabyte/tools/k8s_parent.py"
            else
              k8s_parent=""
            fi && \
            exec ${k8s_parent} /home/yugabyte/bin/yb-master \
              --fs_data_dirs=/mnt/disk0,/mnt/disk1 \
              --master_addresses=yugabyte-yb-master-0.yugabyte-yb-masters.$(NAMESPACE).svc.cluster.local:7100,yugabyte-yb-master-1.yugabyte-yb-masters.$(NAMESPACE).svc.cluster.local:7100,yugabyte-yb-master-2.yugabyte-yb-masters.$(NAMESPACE).svc.cluster.local:7100 \
              --replication_factor=3 \
              --enable_ysql=true \
              --metric_node_name=$(HOSTNAME) \
              --memory_limit_hard_bytes=1824522240 \
              --stderrthreshold=0 \
              --num_cpus=2 \
              --undefok=num_cpus,enable_ysql \
              --default_memory_limit_to_ram_ratio="0.85" \
              --certs_dir=/opt/certs/yugabyte \
              --use_node_to_node_encryption=true \
              --allow_insecure_connections=false \
              --placement_cloud=aws \
              --placement_region=$$NODE_REGION \
              --placement_zone=$$NODE_ZONE \
              --rpc_bind_addresses=$(HOSTNAME).yugabyte-yb-masters.$(NAMESPACE).svc.cluster.local \
              --server_broadcast_addresses=$(HOSTNAME).yugabyte-yb-masters.$(NAMESPACE).svc.cluster.local:7100 \
              --webserver_interface=0.0.0.0
        ports:
          - containerPort: 7000
            name: "http-ui"
          - containerPort: 7100
            name: "tcp-rpc-port"
        volumeMounts:
          - mountPath: /node
            name: node-info
          
          - name: yugabyte-datadir0
            mountPath: /mnt/disk0
          - name: yugabyte-datadir1
            mountPath: /mnt/disk1
          - name: yugabyte-yb-master-tls-cert
            mountPath: /opt/certs/yugabyte
            readOnly: true
          - name: yugabyte-client-tls
            mountPath: /root/.yugabytedb/
            readOnly: true
      - name: yb-cleanup
        image: "yugabytedb/yugabyte:2.15.1.0-b175"
        imagePullPolicy: IfNotPresent
        env:
        - name: USER
          value: "yugabyte"
        command:
          - "/sbin/tini"
          - "--"
        args:
          - "/bin/bash"
          - "-c"
          - >
            while true; do
              sleep 3600;
              /home/yugabyte/scripts/log_cleanup.sh;
            done
        volumeMounts:
          - name: yugabyte-datadir0
            mountPath: /home/yugabyte/
            subPath: yb-data
          - name: yugabyte-datadir0
            mountPath: /var/yugabyte/cores
            subPath: cores

      volumes:
        - name: node-info
          emptyDir: {}
        - name: setup-auto-multi-az-script
          configMap:
            name: yugabyte-setup-auto-multi-az-script
        
        - name: yugabyte-datadir0
          hostPath:
            path: /mnt/disks/ssd0
        - name: yugabyte-datadir1
          hostPath:
            path: /mnt/disks/ssd1
        - name: yugabyte-yb-master-tls-cert
          secret:
            secretName: yugabyte-yb-master-tls-cert
            items:
              - key: tls.crt
                path: node.yugabyte-yb-master-0.yugabyte-yb-masters.yugabyte.svc.cluster.local.crt
              - key: tls.key
                path: node.yugabyte-yb-master-0.yugabyte-yb-masters.yugabyte.svc.cluster.local.key
              - key: tls.crt
                path: node.yugabyte-yb-master-1.yugabyte-yb-masters.yugabyte.svc.cluster.local.crt
              - key: tls.key
                path: node.yugabyte-yb-master-1.yugabyte-yb-masters.yugabyte.svc.cluster.local.key
              - key: tls.crt
                path: node.yugabyte-yb-master-2.yugabyte-yb-masters.yugabyte.svc.cluster.local.crt
              - key: tls.key
                path: node.yugabyte-yb-master-2.yugabyte-yb-masters.yugabyte.svc.cluster.local.key
              - key: ca.crt
                path: ca.crt
            defaultMode: 256
        - name: yugabyte-client-tls
          secret:
            secretName: yugabyte-client-tls
            items:
              - key: ca.crt
                path: root.crt
              - key: tls.crt
                path: yugabytedb.crt
              - key: tls.key
                path: yugabytedb.key
            defaultMode: 256
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/service.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: "yugabyte-yb-tserver"
  namespace: "yugabyte"
  labels:    
    app.kubernetes.io/name: "yb-tserver"    
    heritage: "Helm"
    release: "yugabyte"
    chart: "yugabyte-2.15.1"
    component: "yugabytedb"
spec:
  serviceName: "yugabyte-yb-tservers"
  podManagementPolicy: Parallel
  
  replicas: 3
  
  volumeClaimTemplates:
    - metadata:
        name: yugabyte-datadir0
        labels:          
          heritage: "Helm"
          release: "yugabyte"
          chart: "yugabyte-2.15.1"
          component: "yugabytedb"
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: 10Gi
    - metadata:
        name: yugabyte-datadir1
        labels:          
          heritage: "Helm"
          release: "yugabyte"
          chart: "yugabyte-2.15.1"
          component: "yugabytedb"
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: 10Gi
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      
      partition: 0
      
  selector:
    matchLabels:      
      app.kubernetes.io/name: "yb-tserver"
      release: "yugabyte"
  template:
    metadata:
      annotations:
        checksum/rootCA: 90ace38c3efb4bf1768630295603f06eb0c140b0384c0860e28e0ad1d4dd6a44
      labels:        
        app.kubernetes.io/name: "yb-tserver"        
        heritage: "Helm"
        release: "yugabyte"
        chart: "yugabyte-2.15.1"
        component: "yugabytedb"
    spec: # yb-tservers
      terminationGracePeriodSeconds: 300
      serviceAccountName: yugabyte
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: DoNotSchedule
        labelSelector:
          matchLabels:
            app.kubernetes.io/name: "yb-tserver"
            release: "yugabyte"
      affinity:
        # Set the anti-affinity selector scope to YB masters.
        
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                  - "yb-tserver"
                - key: release
                  operator: In
                  values:
                  - "yugabyte"
              topologyKey: kubernetes.io/hostname
      initContainers:
      # The container that extracts the labels
      - name: get-node-labels
        image: "bitnami/kubectl:1.24.3"
        # It'll put labels here
        volumeMounts:
          - mountPath: /node
            name: node-info
          - name: setup-auto-multi-az-script
            mountPath: "/home/yugabyte/bin/setup-auto-multi-az"
        env:
          # pass node name to the environment
          - name: NODENAME
            valueFrom:
              fieldRef:
                fieldPath: spec.nodeName
          - name: APISERVER
            value: https://kubernetes.default.svc
          - name: SERVICEACCOUNT
            value: /var/run/secrets/kubernetes.io/serviceaccount
        command: ["/bin/bash", "/home/yugabyte/bin/setup-auto-multi-az/get-node-info.sh"]
      containers:
      - name: "yb-tserver"
        image: "yugabytedb/yugabyte:2.15.1.0-b175"
        imagePullPolicy: IfNotPresent
        lifecycle:
          postStart:
            exec:
              command:
                - "bash"
                - "-c"
                - >
                  mkdir -p /mnt/disk0/cores;
                  mkdir -p /mnt/disk0/yb-data/scripts;
                  if [ ! -f /mnt/disk0/yb-data/scripts/log_cleanup.sh ]; then
                    if [ -f /home/yugabyte/bin/log_cleanup.sh ]; then
                      cp /home/yugabyte/bin/log_cleanup.sh /mnt/disk0/yb-data/scripts;
                    fi;
                  fi
        livenessProbe:
          exec:
            command:
            - bash
            - -c
            - touch "/mnt/disk0/disk.check" "/mnt/disk1/disk.check"
          failureThreshold: 3
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 1
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: YBDEVOPS_CORECOPY_DIR
          value: "/mnt/disk0/cores"
        - name: SSL_CERTFILE
          value: /root/.yugabytedb/root.crt
        resources:
        
          limits:
            cpu: 2
            memory: 4Gi
          requests:
            cpu: 2
            memory: 4Gi
        
        # core dumps are collected to workingDir if
        # kernel.core_pattern is set to a relative path like
        # core.%e.%p.%t ref:
        # https://github.com/yugabyte/charts/issues/11
        workingDir: "/mnt/disk0/cores"
        command:
          - "/sbin/tini"
          - "--"
        args:
          - "/bin/bash"
          - "-c"
          - |
            source /node/location && \
            touch "/mnt/disk0/disk.check" "/mnt/disk1/disk.check" && \
            if [ -f /home/yugabyte/tools/k8s_preflight.py ]; then
              PYTHONUNBUFFERED="true" /home/yugabyte/tools/k8s_preflight.py \
                --addr="$(HOSTNAME).yugabyte-yb-tservers.$(NAMESPACE).svc.cluster.local" \
                --port="9100"
            fi && \
            if [ -f /home/yugabyte/tools/k8s_preflight.py ]; then
              PYTHONUNBUFFERED="true" /home/yugabyte/tools/k8s_preflight.py \
                --addr="$(HOSTNAME).yugabyte-yb-tservers.$(NAMESPACE).svc.cluster.local:9100" \
                --port="9100"
            fi && \
            if [ -f /home/yugabyte/tools/k8s_preflight.py ]; then
              PYTHONUNBUFFERED="true" /home/yugabyte/tools/k8s_preflight.py \
                --addr="0.0.0.0" \
                --port="9000"
            fi && \
            if [[ -f /home/yugabyte/tools/k8s_parent.py ]]; then
              k8s_parent="/home/yugabyte/tools/k8s_parent.py"
            else
              k8s_parent=""
            fi && \
            if [ -f /home/yugabyte/tools/k8s_preflight.py ]; then
              PYTHONUNBUFFERED="true" /home/yugabyte/tools/k8s_preflight.py \
                --addr="$(HOSTNAME).yugabyte-yb-tservers.$(NAMESPACE).svc.cluster.local" \
                --port="9042"
            fi && \
            if [ -f /home/yugabyte/tools/k8s_preflight.py ]; then
              PYTHONUNBUFFERED="true" /home/yugabyte/tools/k8s_preflight.py \
                --addr="0.0.0.0:5433" \
                --port="5433"
            fi && \
            exec ${k8s_parent} /home/yugabyte/bin/yb-tserver \
              --fs_data_dirs=/mnt/disk0,/mnt/disk1 \
              --tserver_master_addrs=yugabyte-yb-master-0.yugabyte-yb-masters.$(NAMESPACE).svc.cluster.local:7100,yugabyte-yb-master-1.yugabyte-yb-masters.$(NAMESPACE).svc.cluster.local:7100,yugabyte-yb-master-2.yugabyte-yb-masters.$(NAMESPACE).svc.cluster.local:7100 \
              --metric_node_name=$(HOSTNAME) \
              --memory_limit_hard_bytes=3649044480 \
              --stderrthreshold=0 \
              --num_cpus=2 \
              --undefok=num_cpus,enable_ysql \
              --use_node_hostname_for_local_tserver=true \
              --certs_dir=/opt/certs/yugabyte \
              --use_node_to_node_encryption=true \
              --allow_insecure_connections=false \
              --use_client_to_server_encryption=true \
              --certs_for_client_dir=/opt/certs/yugabyte \
              --rpc_bind_addresses=$(HOSTNAME).yugabyte-yb-tservers.$(NAMESPACE).svc.cluster.local \
              --server_broadcast_addresses=$(HOSTNAME).yugabyte-yb-tservers.$(NAMESPACE).svc.cluster.local:9100 \
              --webserver_interface=0.0.0.0 \
              --enable_ysql=true \
              --pgsql_proxy_bind_address=0.0.0.0:5433 \
              --placement_cloud=aws \
              --placement_region=$$NODE_REGION \
              --placement_zone=$$NODE_ZONE \
              --cql_proxy_bind_address=$(HOSTNAME).yugabyte-yb-tservers.$(NAMESPACE).svc.cluster.local
        ports:
          - containerPort: 9000
            name: "http-ui"
          - containerPort: 12000
            name: "http-ycql-met"
          - containerPort: 11000
            name: "http-yedis-met"
          - containerPort: 13000
            name: "http-ysql-met"
          - containerPort: 9100
            name: "tcp-rpc-port"
          - containerPort: 6379
            name: "tcp-yedis-port"
          - containerPort: 9042
            name: "tcp-yql-port"
          - containerPort: 5433
            name: "tcp-ysql-port"
        volumeMounts:
          - mountPath: /node
            name: node-info
          
          - name: yugabyte-datadir0
            mountPath: /mnt/disk0
          - name: yugabyte-datadir1
            mountPath: /mnt/disk1
          - name: yugabyte-yb-tserver-tls-cert
            mountPath: /opt/certs/yugabyte
            readOnly: true
          - name: yugabyte-client-tls
            mountPath: /root/.yugabytedb/
            readOnly: true
      - name: yb-cleanup
        image: "yugabytedb/yugabyte:2.15.1.0-b175"
        imagePullPolicy: IfNotPresent
        env:
        - name: USER
          value: "yugabyte"
        command:
          - "/sbin/tini"
          - "--"
        args:
          - "/bin/bash"
          - "-c"
          - >
            while true; do
              sleep 3600;
              /home/yugabyte/scripts/log_cleanup.sh;
            done
        volumeMounts:
          - name: yugabyte-datadir0
            mountPath: /home/yugabyte/
            subPath: yb-data
          - name: yugabyte-datadir0
            mountPath: /var/yugabyte/cores
            subPath: cores

      volumes:
        - name: node-info
          emptyDir: {}
        - name: setup-auto-multi-az-script
          configMap:
            name: yugabyte-setup-auto-multi-az-script
        
        - name: yugabyte-datadir0
          hostPath:
            path: /mnt/disks/ssd0
        - name: yugabyte-datadir1
          hostPath:
            path: /mnt/disks/ssd1
        - name: yugabyte-yb-tserver-tls-cert
          secret:
            secretName: yugabyte-yb-tserver-tls-cert
            items:
              - key: tls.crt
                path: node.yugabyte-yb-tserver-0.yugabyte-yb-tservers.yugabyte.svc.cluster.local.crt
              - key: tls.key
                path: node.yugabyte-yb-tserver-0.yugabyte-yb-tservers.yugabyte.svc.cluster.local.key
              - key: tls.crt
                path: node.yugabyte-yb-tserver-1.yugabyte-yb-tservers.yugabyte.svc.cluster.local.crt
              - key: tls.key
                path: node.yugabyte-yb-tserver-1.yugabyte-yb-tservers.yugabyte.svc.cluster.local.key
              - key: tls.crt
                path: node.yugabyte-yb-tserver-2.yugabyte-yb-tservers.yugabyte.svc.cluster.local.crt
              - key: tls.key
                path: node.yugabyte-yb-tserver-2.yugabyte-yb-tservers.yugabyte.svc.cluster.local.key
              - key: ca.crt
                path: ca.crt
            defaultMode: 256
        - name: yugabyte-client-tls
          secret:
            secretName: yugabyte-client-tls
            items:
              - key: ca.crt
                path: root.crt
              - key: tls.crt
                path: yugabytedb.crt
              - key: tls.key
                path: yugabytedb.key
            defaultMode: 256
---
# Source: yugabyte/templates/application.yaml
apiVersion: app.k8s.io/v1beta1
kind: Application
metadata:
  name: yugabyte
spec:
  selector:
    matchLabels: {}
  componentKinds:
  - group: v1
    kind: Service
  - group: networking.k8s.io
    kind: Ingress
  - group: cert-manager.io
    kind: Certificate
  - group: apps
    kind: StatefulSet
  - group: apps
    kind: Deployment
  - group: batch
    kind: CronJob
  - group: batch
    kind: Job
  descriptor:
    type: yugabyte
    version: 0.1.0
    description: yugabyte deployed on plural
    icons:
    - src: https://plural-assets.s3.us-east-2.amazonaws.com/uploads/repos/4036bea8-d1bd-4720-b0e6-3e9d91706aad/yugabyte.png?v=63826061362
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/certificates.yaml
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: yugabyte-ca
  namespace: "yugabyte"
spec:
  isCA: true
  privateKey:
    algorithm: "ECDSA"
    encoding: PKCS8
    size: 521
  commonName: Yugabyte Selfsigned CA
  secretName: yugabyte-ca
  issuerRef:
    name: "yugabyte-selfsigned-bootstrap"
    kind: Issuer
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/certificates.yaml
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: yugabyte-yb-master-tls-cert
  namespace: "yugabyte"
spec:
  secretTemplate:
    labels:      
      app.kubernetes.io/name: "yb-master"      
      heritage: "Helm"
      release: "yugabyte"
      chart: "yugabyte-2.15.1"
      component: "yugabytedb"
  issuerRef:
    name:  "yugabyte-selfsigned"
    kind: Issuer
  secretName: yugabyte-yb-master-tls-cert
  duration: "2160h"
  renewBefore: "360h"
  commonName: yugabyte-yb-masters
  isCA: false
  privateKey:
    algorithm: "ECDSA"
    encoding: PKCS8
    size: 521
  usages:
    - server auth
    - client auth
  # At least one of a DNS Name, URI, or IP address is required.
  dnsNames:
  - yugabyte-yb-master-0.yugabyte-yb-masters.yugabyte.svc.cluster.local
  - yugabyte-yb-master-1.yugabyte-yb-masters.yugabyte.svc.cluster.local
  - yugabyte-yb-master-2.yugabyte-yb-masters.yugabyte.svc.cluster.local
  uris: []
  ipAddresses: []
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/certificates.yaml
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: yugabyte-yb-tserver-tls-cert
  namespace: "yugabyte"
spec:
  secretTemplate:
    labels:      
      app.kubernetes.io/name: "yb-tserver"      
      heritage: "Helm"
      release: "yugabyte"
      chart: "yugabyte-2.15.1"
      component: "yugabytedb"
  issuerRef:
    name:  "yugabyte-selfsigned"
    kind: Issuer
  secretName: yugabyte-yb-tserver-tls-cert
  duration: "2160h"
  renewBefore: "360h"
  commonName: yugabyte-yb-tservers
  isCA: false
  privateKey:
    algorithm: "ECDSA"
    encoding: PKCS8
    size: 521
  usages:
    - server auth
    - client auth
  # At least one of a DNS Name, URI, or IP address is required.
  dnsNames:
  - yugabyte-yb-tserver-0.yugabyte-yb-tservers.yugabyte.svc.cluster.local
  - yugabyte-yb-tserver-1.yugabyte-yb-tservers.yugabyte.svc.cluster.local
  - yugabyte-yb-tserver-2.yugabyte-yb-tservers.yugabyte.svc.cluster.local
  uris: []
  ipAddresses: []
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/certificates.yaml
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: yugabyte-client-tls
  namespace: "yugabyte"
spec:
  secretTemplate:
    labels:      
      heritage: "Helm"
      release: "yugabyte"
      chart: "yugabyte-2.15.1"
      component: "yugabytedb"
  issuerRef:
    name:  "yugabyte-selfsigned"
    kind: Issuer
  secretName:  yugabyte-client-tls
  duration: "2160h"
  renewBefore: "360h"
  commonName: yugabyte
  isCA: false
  privateKey:
    algorithm: "ECDSA"
    encoding: PKCS8
    size: 521
  usages:
    - client auth
  dnsNames: []
  uris: []
  ipAddresses: []
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/certificates.yaml
---
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name:  "yugabyte-selfsigned-bootstrap"
  namespace: "yugabyte"
spec:
  selfSigned: {}
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/certificates.yaml
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: "yugabyte-selfsigned"
  namespace: "yugabyte"
spec:
  ca:
    secretName: yugabyte-ca
---
# Source: yugabyte/templates/license.yaml
apiVersion: platform.plural.sh/v1alpha1
kind: License
metadata:
  name: yugabyte
spec:
  secretRef:
    name: plural-license-secret
    key: license
---
# Source: yugabyte/charts/yugabyte/charts/oidc-config/templates/servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    helm.sh/chart: oidc-config-0.1.6
    app.kubernetes.io/name: oidc-config
    app.kubernetes.io/instance: yugabyte
    app.kubernetes.io/version: "1.16.0"
    app.kubernetes.io/managed-by: Helm
  name: oauth2-proxy-service-monitor
spec:
  endpoints:
  - interval: 5s
    path: /metrics
    port: metrics-oauth
  namespaceSelector:
    matchNames:
    - yugabyte
  selector:
    matchLabels:
      endpoint: oauth2-proxy
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/master-servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: yugabyte-yb-master
  labels:
    app.kubernetes.io/name: "yb-master"
    release: "yugabyte"
    chart: "yugabyte"
    component: "yugabytedb"
spec:
  jobLabel: "release"
  selector:
    matchLabels:
      app.kubernetes.io/name: "yb-master"
      release: "yugabyte"
      service-type: "headless"
  endpoints:
  - port: http-ui
    path: /prometheus-metrics
    interval: 30s
    relabelings:
    - targetLabel: "group"
      replacement: "yb-master"
    - targetLabel: "export_type"
      replacement: "master_export"
    - targetLabel: "node_prefix"
      replacement: "yugabyte"
    metricRelabelings:
    - regex: (.*)
      replacement: $1
      sourceLabels:
      - __name__
      targetLabel: saved_name
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(.*)
      replacement: $1
      sourceLabels:
      - __name__
      targetLabel: server_type
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(.*)
      replacement: $2
      sourceLabels:
      - __name__
      targetLabel: service_type
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(_sum|_count)?
      replacement: $3
      sourceLabels:
      - __name__
      targetLabel: service_method
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(_sum|_count)?
      replacement: rpc_latency$4
      sourceLabels:
      - __name__
      targetLabel: __name__
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/tserver-servicemonitor.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: yugabyte-yb-tserver
  labels:
    app.kubernetes.io/name: "yb-tserver"
    release: "yugabyte"
    chart: "yugabyte"
    component: "yugabytedb"
spec:
  jobLabel: "release"
  selector:
    matchLabels:
      app.kubernetes.io/name: "yb-tserver"
      release: "yugabyte"
      service-type: "headless"
  endpoints:
  - port: http-ui
    path: /prometheus-metrics
    interval: 30s
    relabelings:
    - targetLabel: "group"
      replacement: "yb-tserver"
    - targetLabel: "export_type"
      replacement: "tserver_export"
    - targetLabel: "node_prefix"
      replacement: "yugabyte"
    metricRelabelings:
    - regex: (.*)
      replacement: $1
      sourceLabels:
      - __name__
      targetLabel: saved_name
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(.*)
      replacement: $1
      sourceLabels:
      - __name__
      targetLabel: server_type
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(.*)
      replacement: $2
      sourceLabels:
      - __name__
      targetLabel: service_type
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(_sum|_count)?
      replacement: $3
      sourceLabels:
      - __name__
      targetLabel: service_method
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(_sum|_count)?
      replacement: rpc_latency$4
      sourceLabels:
      - __name__
      targetLabel: __name__
  - port: http-ycql-met
    path: /prometheus-metrics
    interval: 30s
    relabelings:
    - targetLabel: "group"
      replacement: "ycql"
    - targetLabel: "export_type"
      replacement: "cql_export"
    - targetLabel: "node_prefix"
      replacement: "yugabyte"
    metricRelabelings:
    - regex: (.*)
      replacement: $1
      sourceLabels:
      - __name__
      targetLabel: saved_name
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(.*)
      replacement: $1
      sourceLabels:
      - __name__
      targetLabel: server_type
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(.*)
      replacement: $2
      sourceLabels:
      - __name__
      targetLabel: service_type
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(_sum|_count)?
      replacement: $3
      sourceLabels:
      - __name__
      targetLabel: service_method
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(_sum|_count)?
      replacement: rpc_latency$4
      sourceLabels:
      - __name__
      targetLabel: __name__
  - port: http-ysql-met
    path: /prometheus-metrics
    interval: 30s
    relabelings:
    - targetLabel: "group"
      replacement: "ysql"
    - targetLabel: "export_type"
      replacement: "ysql_export"
    - targetLabel: "node_prefix"
      replacement: "yugabyte"
    metricRelabelings:
    - regex: (.*)
      replacement: $1
      sourceLabels:
      - __name__
      targetLabel: saved_name
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(.*)
      replacement: $1
      sourceLabels:
      - __name__
      targetLabel: server_type
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(.*)
      replacement: $2
      sourceLabels:
      - __name__
      targetLabel: service_type
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(_sum|_count)?
      replacement: $3
      sourceLabels:
      - __name__
      targetLabel: service_method
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(_sum|_count)?
      replacement: rpc_latency$4
      sourceLabels:
      - __name__
      targetLabel: __name__
  - port: http-yedis-met
    path: /prometheus-metrics
    interval: 30s
    relabelings:
    - targetLabel: "group"
      replacement: "yedis"
    - targetLabel: "export_type"
      replacement: "redis_export"
    - targetLabel: "node_prefix"
      replacement: "yugabyte"
    metricRelabelings:
    - regex: (.*)
      replacement: $1
      sourceLabels:
      - __name__
      targetLabel: saved_name
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(.*)
      replacement: $1
      sourceLabels:
      - __name__
      targetLabel: server_type
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(.*)
      replacement: $2
      sourceLabels:
      - __name__
      targetLabel: service_type
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(_sum|_count)?
      replacement: $3
      sourceLabels:
      - __name__
      targetLabel: service_method
    - regex: handler_latency_(yb_[^_]*)_([^_]*)_([^_]*)(_sum|_count)?
      replacement: rpc_latency$4
      sourceLabels:
      - __name__
      targetLabel: __name__
---
# Source: yugabyte/charts/yugabyte/charts/yugabyte/templates/hooks/setup-auto-multi-az-job.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: yugabyte-setup-auto-multi-az
  namespace: "yugabyte"
  labels:
    app: "setup-auto-multi-az"
    release: "yugabyte"
    chart: "yugabyte"
    component: "yugabytedb"
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-weight": "10"
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  backoffLimit: 2
  template:
    metadata:
      name: "setup-auto-multi-az"
      labels:
        app: "setup-auto-multi-az"
        release: "yugabyte"
        chart: "yugabyte"
        component: "yugabytedb"
    spec:
      serviceAccountName: yugabyte
      restartPolicy: Never
      containers:
      - name: setup-auto-multi-az
        image: "bitnami/kubectl:1.24.3"
        command:
        - 'bash'
        - '/home/yugabyte/bin/setup-auto-multi-az/setup-auto-multi-az.sh'
        volumeMounts:
        - name: setup-auto-multi-az-script
          mountPath: "/home/yugabyte/bin/setup-auto-multi-az"
      volumes:
      - name: setup-auto-multi-az-script
        configMap:
          name: yugabyte-setup-auto-multi-az-script
